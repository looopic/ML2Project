{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMeLLgmVeYLqGAq7KdIzslO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/looopic/ML2Project/blob/main/ML2Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual text recognition model by Carlo Huser (husercar@students.zhaw.ch)\n",
        "\n",
        "\n",
        "This model is used to extract the ingredients from an image of groceries. I want to use this model to build an app to detect allergies or intolerances on specific ingredients."
      ],
      "metadata": {
        "id": "SycYOONrmClP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "I was thinking of making my own dataset for this problem, but I quickly found out, that it's too time consuming to annotate every single picture on my own.\n",
        "Therefore I searched the internet for a dataset I could use and found the TextOCR dataset on kaggle (https://www.kaggle.com/datasets/robikscube/textocr-text-extraction-from-images-dataset?select=annot.csv)\n",
        "\n",
        "Please use one of the two options below to download the dataset to the /content-folder. For the kaggle-option, you need to download your api-json first."
      ],
      "metadata": {
        "id": "Y6xGoGSZnEXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1zg0ofaVhgnJQ4bmfLeo5MG1iAtoT2HnO\n",
        "!unzip OCR_data.zip"
      ],
      "metadata": {
        "id": "peOEjFwh2aKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload your kaggle api json-file\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "\n",
        "!kaggle datasets download -d robikscube/textocr-text-extraction-from-images-dataset\n",
        "!unzip textocr-text-extraction-from-images-dataset.zip\n"
      ],
      "metadata": {
        "id": "BEswzSwl897Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Images\n",
        "\n",
        "Due to the images being real photographs, there are multiple words on each image. I need to extract those words on their own, so that i can label that word.\n",
        "\n",
        "ATTENTION! The whole preprocessing can last up to 10 hours! You can interrupt the preprocessing earlier if you want to."
      ],
      "metadata": {
        "id": "vNxVpYuBCAGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kHMsBTHfKClN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roundup(x):\n",
        "    return int(math.ceil(x / 10.0)) * 10"
      ],
      "metadata": {
        "id": "YftwycerKLLB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "\told_h,old_w=img.shape[0],img.shape[1]\n",
        "  \n",
        "\t#Pad the height.\n",
        "\t#If height is less than 512 then pad to 512\n",
        "\tif old_h<512:\n",
        "\t\tto_pad=np.ones((512-old_h,old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad))\n",
        "\t\tnew_height=512\n",
        "\telse:\n",
        "\t#If height >512 then pad to nearest 10.\n",
        "\t\tto_pad=np.ones((roundup(old_h)-old_h,old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad))\n",
        "\t\tnew_height=roundup(old_h)\n",
        "\n",
        "\t#Pad the width.\n",
        "\tif old_w<512:\n",
        "\t\tto_pad=np.ones((new_height,512-old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad),axis=1)\n",
        "\t\tnew_width=512\n",
        "\telse:\n",
        "\t\tto_pad=np.ones((new_height,roundup(old_w)-old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad),axis=1)\n",
        "\t\tnew_width=roundup(old_w)-old_w\n",
        "\treturn img"
      ],
      "metadata": {
        "id": "mF4yGvinI3mE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize arrays to store data and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Create directories to store the images\n",
        "image_dir = '/content/preprocessed_images/'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# remove all pictures in preprocessed\n",
        "image_dir = '/content/preprocessed_images/'\n",
        "file_list = os.listdir(image_dir)\n",
        "for file_name in file_list:\n",
        "    file_path = os.path.join(image_dir, file_name)\n",
        "    os.remove(file_path)\n",
        "\n",
        "# read annotations csv file\n",
        "with open('/content/annot.csv','r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  \n",
        "  #skip header\n",
        "  next(reader)\n",
        "\n",
        "  for row in tqdm(reader):\n",
        "    image_path = \"/content/train_val_images/train_images/\"+row[2]+\".jpg\"\n",
        "    boundary = row[5].strip('[]').split(',')\n",
        "    boundaryArray = [float(value) for value in boundary]\n",
        "    boundaryXmin = int(min(boundaryArray[0], boundaryArray[2], boundaryArray[4], boundaryArray[6]))\n",
        "    boundaryXmax = int(max(boundaryArray[0], boundaryArray[2], boundaryArray[4], boundaryArray[6]))\n",
        "    boundaryYmin = int(min(boundaryArray[1], boundaryArray[3], boundaryArray[5], boundaryArray[7]))\n",
        "    boundaryYmax = int(max(boundaryArray[1], boundaryArray[3], boundaryArray[5], boundaryArray[7]))\n",
        "    text = row[4]\n",
        "\n",
        "    #load image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    #extract text region\n",
        "    text_region = image[boundaryYmin:boundaryYmax, boundaryXmin:boundaryXmax]\n",
        "\n",
        "    #preprocess text region\n",
        "    text_region = preprocess(text_region)\n",
        "\n",
        "    # Generate a unique filename for each image\n",
        "    image_filename = f\"{row[2]+text}.jpg\"\n",
        "\n",
        "    # Save the preprocessed image to disk\n",
        "    image_path = os.path.join(image_dir, image_filename)\n",
        "    cv2.imwrite(image_path, text_region)\n",
        "\n",
        "    #append data to arrays\n",
        "    images.append(image_filename)\n",
        "    labels.append(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6sn_VI1HCggr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)\n",
        "encoded_labels = label_encoder.transform(labels)"
      ],
      "metadata": {
        "id": "FdnhHqijY82W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "I'm following the tutorial on medium.com (https://medium.com/geekculture/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b)\n",
        "But due to my data not being compiled the same as the data this tutorial uses, I had to adjust practically the whole data preprocessing."
      ],
      "metadata": {
        "id": "HDutK2Jhm_BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import gc"
      ],
      "metadata": {
        "id": "kW_1rM5VVrZG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "random_indices = np.arange(len(images))\n",
        "num_samples = len(images)\n",
        "num_batches = num_samples // batch_size\n",
        "num_samples = num_batches * batch_size\n",
        "random_indices = random_indices[:num_samples]\n",
        "np.random.shuffle(random_indices)\n",
        "training_indices=random_indices[0:int(0.9*len(random_indices))]\n",
        "test_indices=random_indices[int(0.9*len(random_indices)):]\n",
        "#X_test = []\n",
        "#for filename in images[test_indices]:\n",
        "#  img=cv2.imread(f'/content/preprocessed_images/{fn}.jpg',0)\n",
        "#  X_test.append(img)\n",
        "#y_test = encoded_labels[test_indices]\n",
        "\n",
        "def batch_generator(indices, images, encoded_labels, batch_size):\n",
        "    while True:\n",
        "        X = []\n",
        "        Y = []\n",
        "        for i in range(batch_size):\n",
        "            index = random.choice(indices)\n",
        "            fn = images[index]\n",
        "            img = cv2.imread(f'/content/preprocessed_images/{fn}', cv2.IMREAD_COLOR)\n",
        "            label = encoded_labels[index]\n",
        "            X.append(img)\n",
        "            Y.append(label)\n",
        "            \n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "\n",
        "        print(X.shape, Y.shape)\n",
        "\n",
        "        yield X, Y\n"
      ],
      "metadata": {
        "id": "j60_bT69V29X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet(pretrained_weights = None,input_size = (512,512,3)):\n",
        "  inputs = Input(input_size)\n",
        "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  drop4 = Dropout(0.5)(conv4)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "  drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "  up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "  merge6 = concatenate([drop4,up6], axis = 3)\n",
        "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "  up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "  merge7 = concatenate([conv3,up7], axis = 3)\n",
        "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "  up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "  merge8 = concatenate([conv2,up8], axis = 3)\n",
        "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "  up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "  merge9 = concatenate([conv1,up9], axis = 3)\n",
        "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "  conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "  conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "  model = Model(inputs,conv10)\n",
        "\n",
        "  model.compile(optimizer = Adam(learning_rate=0.001), loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics = ['accuracy'])\n",
        "  \n",
        "  #model.summary()\n",
        "\n",
        "  if(pretrained_weights):\n",
        "    model.load_weights(pretrained_weights)\n",
        "\n",
        "  return model\n",
        "\n",
        "model=unet()\n",
        "\n",
        "\n",
        "mc = ModelCheckpoint('/content/checkpoints/weights{epoch:08d}.h5',save_weights_only=True, save_freq=4)\n",
        "ec = EarlyStopping(patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "GSnzuZioMD2U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 5\n",
        "train_generator = batch_generator(training_indices, images, encoded_labels, batch_size)\n",
        "test_generator = batch_generator(test_indices, images, encoded_labels, batch_size)\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "# Clear the memory after each epoch\n",
        "def clear_memory():\n",
        "    del train_generator\n",
        "    del test_generator\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_generator, \n",
        "        steps_per_epoch=len(training_indices) // batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=len(test_indices) // batch_size,\n",
        "        callbacks=[mc, ec],\n",
        "        shuffle=True\n",
        "   )\n",
        "\n",
        "    # Clear the memory\n",
        "    clear_memory()\n",
        "\n",
        "\n",
        "#model.evaluate(X_test, y_test)\n",
        "model.save('/content/ocr_model.h5')"
      ],
      "metadata": {
        "id": "OLN_8itrWCaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Comparision"
      ],
      "metadata": {
        "id": "wtwoGk-qyOJE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJTO4xRsyW-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}