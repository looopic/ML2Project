{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqGqBnb9knOvV6xNigbASu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/looopic/ML2Project/blob/main/ML2Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual text recognition model by Carlo Huser (husercar@students.zhaw.ch)\n",
        "\n",
        "\n",
        "This model is used to extract the ingredients from an image of groceries. I want to use this model to build an app to detect allergies or intolerances on specific ingredients."
      ],
      "metadata": {
        "id": "SycYOONrmClP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "I was thinking of making my own dataset for this problem, but I quickly found out, that it's too time consuming to annotate every single picture on my own.\n",
        "Therefore I searched the internet for a dataset I could use and found the TextOCR dataset on kaggle (https://www.kaggle.com/datasets/robikscube/textocr-text-extraction-from-images-dataset?select=annot.csv)"
      ],
      "metadata": {
        "id": "Y6xGoGSZnEXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "shutil.rmtree('/content', ignore_errors=True)\n",
        "os.makedirs('/content', exist_ok=True)"
      ],
      "metadata": {
        "id": "szTrnMZGSBio"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1zg0ofaVhgnJQ4bmfLeo5MG1iAtoT2HnO\n",
        "!unzip OCR_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peOEjFwh2aKe",
        "outputId": "c6a8b5dd-e31e-4da1-c850-a604cb64452d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gdown/cli.py\", line 151, in main\n",
            "    filename = download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gdown/download.py\", line 247, in download\n",
            "    f = open(tmp_file, \"ab\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'OCR_data.zip334ck2q1tmp'\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "unzip:  cannot find or open OCR_data.zip, OCR_data.zip.zip or OCR_data.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Images\n",
        "\n",
        "Due to the images being real photographs, there are multiple words on each image. I need to extract those word on their own, so that i can label that word."
      ],
      "metadata": {
        "id": "vNxVpYuBCAGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kHMsBTHfKClN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roundup(x):\n",
        "    return int(math.ceil(x / 10.0)) * 10"
      ],
      "metadata": {
        "id": "YftwycerKLLB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "\told_h,old_w=img.shape[0],img.shape[1]\n",
        "  \n",
        "\t#Pad the height.\n",
        "\t#If height is less than 512 then pad to 512\n",
        "\tif old_h<512:\n",
        "\t\tto_pad=np.ones((512-old_h,old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad))\n",
        "\t\tnew_height=512\n",
        "\telse:\n",
        "\t#If height >512 then pad to nearest 10.\n",
        "\t\tto_pad=np.ones((roundup(old_h)-old_h,old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad))\n",
        "\t\tnew_height=roundup(old_h)\n",
        "\n",
        "\t#Pad the width.\n",
        "\tif old_w<512:\n",
        "\t\tto_pad=np.ones((new_height,512-old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad),axis=1)\n",
        "\t\tnew_width=512\n",
        "\telse:\n",
        "\t\tto_pad=np.ones((new_height,roundup(old_w)-old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad),axis=1)\n",
        "\t\tnew_width=roundup(old_w)-old_w\n",
        "\treturn img"
      ],
      "metadata": {
        "id": "mF4yGvinI3mE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize arrays to store data and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Create directories to store the images\n",
        "image_dir = '/content/preprocessed_images/'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# read annotations csv file\n",
        "with open('/content/annot.csv','r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  \n",
        "  #skip header\n",
        "  next(reader)\n",
        "\n",
        "  for row in tqdm(reader):\n",
        "    image_path = \"/content/train_val_images/train_images/\"+row[2]+\".jpg\"\n",
        "    boundary = row[5].strip('[]').split(',')\n",
        "    boundaryArray = [float(value) for value in boundary]\n",
        "    boundaryXmin = int(min(boundaryArray[0], boundaryArray[2], boundaryArray[4], boundaryArray[6]))\n",
        "    boundaryXmax = int(max(boundaryArray[0], boundaryArray[2], boundaryArray[4], boundaryArray[6]))\n",
        "    boundaryYmin = int(min(boundaryArray[1], boundaryArray[3], boundaryArray[5], boundaryArray[7]))\n",
        "    boundaryYmax = int(max(boundaryArray[1], boundaryArray[3], boundaryArray[5], boundaryArray[7]))\n",
        "    text = row[4]\n",
        "\n",
        "    #load image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    #extract text region\n",
        "    text_region = image[boundaryYmin:boundaryYmax, boundaryXmin:boundaryXmax]\n",
        "\n",
        "    #preprocess text region\n",
        "    text_region = preprocess(text_region)\n",
        "\n",
        "    # Generate a unique filename for each image\n",
        "    image_filename = f\"{row[2]+text}.jpg\"\n",
        "\n",
        "    # Save the preprocessed image to disk\n",
        "    image_path = os.path.join(image_dir, image_filename)\n",
        "    cv2.imwrite(image_path, text_region)\n",
        "\n",
        "    #append data to arrays\n",
        "    images.append(image_filename)\n",
        "    labels.append(text)\n",
        "\n",
        "#encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)\n",
        "encoded_labels = label_encoder.transform(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "6sn_VI1HCggr",
        "outputId": "0456616d-a1be-405f-eaf4-16c083fd90f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "197it [00:07, 28.04it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c81c75940267>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Save the preprocessed image to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_region\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#append data to arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "I'm following the tutorial on medium.com (https://medium.com/geekculture/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b)"
      ],
      "metadata": {
        "id": "HDutK2Jhm_BX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSnzuZioMD2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Comparision"
      ],
      "metadata": {
        "id": "wtwoGk-qyOJE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJTO4xRsyW-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}