{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/looopic/ML2Project/blob/main/ML2Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SycYOONrmClP"
      },
      "source": [
        "# Visual text recognition model by Carlo Huser (husercar@students.zhaw.ch)\n",
        "\n",
        "\n",
        "This model is used to extract the ingredients from an image of groceries. I want to use this model to build an app to detect allergies or intolerances on specific ingredients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6xGoGSZnEXv"
      },
      "source": [
        "# Data\n",
        "I was thinking of making my own dataset for this problem, but I quickly found out, that it's too time consuming to annotate every single picture on my own.\n",
        "Therefore I searched the internet for a dataset I could use and found the TextOCR dataset on kaggle (https://www.kaggle.com/datasets/robikscube/textocr-text-extraction-from-images-dataset?select=annot.csv)\n",
        "\n",
        "Please use one of the two options below to download the dataset to the /content-folder. For the kaggle-option, you need to download your api-json first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peOEjFwh2aKe"
      },
      "outputs": [],
      "source": [
        "!gdown 1zg0ofaVhgnJQ4bmfLeo5MG1iAtoT2HnO\n",
        "!unzip OCR_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEswzSwl897Y"
      },
      "outputs": [],
      "source": [
        "# upload your kaggle api json-file\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "\n",
        "!kaggle datasets download -d robikscube/textocr-text-extraction-from-images-dataset\n",
        "!unzip textocr-text-extraction-from-images-dataset.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNxVpYuBCAGA"
      },
      "source": [
        "# Preprocess Images\n",
        "\n",
        "Due to the images being real photographs, there are multiple words on each image. I need to extract those words on their own, so that i can label that word.\n",
        "\n",
        "ATTENTION! The whole preprocessing can last up to 10 hours! You can interrupt the preprocessing earlier if you want to. You can choose how many you'd like to preprocess by changing the num_interations variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kHMsBTHfKClN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YftwycerKLLB"
      },
      "outputs": [],
      "source": [
        "def roundup(x):\n",
        "    return int(math.ceil(x / 10.0)) * 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mF4yGvinI3mE"
      },
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "\told_h,old_w=img.shape[0],img.shape[1]\n",
        "  \n",
        "\t#Pad the height.\n",
        "\t#If height is less than 512 then pad to 512\n",
        "\tif old_h<512:\n",
        "\t\tto_pad=np.ones((512-old_h,old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad))\n",
        "\t\tnew_height=512\n",
        "\telse:\n",
        "\t#If height >512 then pad to nearest 10.\n",
        "\t\tto_pad=np.ones((roundup(old_h)-old_h,old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad))\n",
        "\t\tnew_height=roundup(old_h)\n",
        "\n",
        "\t#Pad the width.\n",
        "\tif old_w<512:\n",
        "\t\tto_pad=np.ones((new_height,512-old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad),axis=1)\n",
        "\t\tnew_width=512\n",
        "\telse:\n",
        "\t\tto_pad=np.ones((new_height,roundup(old_w)-old_w,3))*255\n",
        "\t\timg=np.concatenate((img,to_pad),axis=1)\n",
        "\t\tnew_width=roundup(old_w)-old_w\n",
        "\treturn img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sn_VI1HCggr"
      },
      "outputs": [],
      "source": [
        "# initialize arrays to store data and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "num_iterations = 2048\n",
        "\n",
        "# Create directories to store the images\n",
        "image_dir = '/content/preprocessed_images/'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# remove all pictures in preprocessed\n",
        "image_dir = '/content/preprocessed_images/'\n",
        "file_list = os.listdir(image_dir)\n",
        "for file_name in file_list:\n",
        "    file_path = os.path.join(image_dir, file_name)\n",
        "    os.remove(file_path)\n",
        "\n",
        "# read annotations csv file\n",
        "with open('/content/annot.csv','r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  \n",
        "  #skip header\n",
        "  next(reader)\n",
        "\n",
        "  iterations = 0\n",
        "\n",
        "  for row in tqdm(reader):\n",
        "    image_path = \"/content/train_val_images/train_images/\"+row[2]+\".jpg\"\n",
        "    boundary = row[5].strip('[]').split(',')\n",
        "    boundaryArray = [float(value) for value in boundary]\n",
        "    boundaryXmin = int(min(boundaryArray[0], boundaryArray[2], boundaryArray[4], boundaryArray[6]))\n",
        "    boundaryXmax = int(max(boundaryArray[0], boundaryArray[2], boundaryArray[4], boundaryArray[6]))\n",
        "    boundaryYmin = int(min(boundaryArray[1], boundaryArray[3], boundaryArray[5], boundaryArray[7]))\n",
        "    boundaryYmax = int(max(boundaryArray[1], boundaryArray[3], boundaryArray[5], boundaryArray[7]))\n",
        "    text = row[4]\n",
        "    name = row[0]\n",
        "\n",
        "    # Check if the text contains non-Roman letters or numbers\n",
        "    # Skips everything containing anything other than roman letters, roman digits, whitespaces and punctuation\n",
        "    if not re.match(r'^[a-zA-Z0-9\\s' + re.escape(string.punctuation) + ']+$', text):\n",
        "            continue\n",
        "\n",
        "    #load image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    #extract text region\n",
        "    text_region = image[boundaryYmin:boundaryYmax, boundaryXmin:boundaryXmax]\n",
        "\n",
        "    #preprocess text region\n",
        "    text_region = preprocess(text_region)\n",
        "\n",
        "    # Generate a unique filename for each image\n",
        "    image_filename = f\"{name}.jpg\"\n",
        "\n",
        "    # Save the preprocessed image to disk\n",
        "    image_path = os.path.join(image_dir, image_filename)\n",
        "    cv2.imwrite(image_path, text_region)\n",
        "\n",
        "    #append data to arrays\n",
        "    images.append(image_filename)\n",
        "    labels.append(text)\n",
        "\n",
        "    if iterations == num_iterations:\n",
        "            break\n",
        "    iterations += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdnhHqijY82W"
      },
      "outputs": [],
      "source": [
        "#encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)\n",
        "encoded_labels = label_encoder.transform(labels)\n",
        "\n",
        "print(\"Length Labels:\",len(encoded_labels))\n",
        "print(\"Length Images:\",len(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDutK2Jhm_BX"
      },
      "source": [
        "# Model\n",
        "\n",
        "I'm following the tutorial on medium.com (https://medium.com/geekculture/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b)\n",
        "but due to my data not being compiled the same as the data this tutorial uses, I had to adjust practically the whole data preprocessing.\n",
        "\n",
        "The training on colab will take up to 24 hours. This is due to the memory intensity of an OCR model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_1rM5VVrZG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import Sequential\n",
        "from keras import metrics\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GSnzuZioMD2U"
      },
      "outputs": [],
      "source": [
        "def unet(pretrained_weights=None, input_size=(128, 128, 1)):\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop5))\n",
        "    merge6 = concatenate([drop4, up6], axis=3)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
        "\n",
        "    up7 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
        "    merge7 = concatenate([conv3, up7], axis=3)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
        "\n",
        "    up8 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
        "    merge8 = concatenate([conv2, up8], axis=3)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
        "\n",
        "    up9 = Conv2D(32, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
        "    merge9 = concatenate([conv1, up9], axis=3)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
        "    \n",
        "    flatten = Flatten()(conv9)\n",
        "    \n",
        "    conv10 = Dense(num_classes, activation='softmax')(flatten)\n",
        "\n",
        "    model = Model(inputs, conv10)\n",
        "\n",
        "    model.compile(optimizer=Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "    if pretrained_weights:\n",
        "        model.load_weights(pretrained_weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = unet()\n",
        "\n",
        "\n",
        "mc = ModelCheckpoint('/content/drive/MyDrive/ML2/weightsCP.h5',save_weights_only=True, save_freq=1,verbose = 1)\n",
        "ec = EarlyStopping(patience=3, restore_best_weights=True, monitor=\"loss\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mOggvej_aPOP"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from image paths and labels\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, encoded_labels))\n",
        "\n",
        "# Define preprocessing and augmentation functions\n",
        "def preprocess_fn(image_path, label):\n",
        "\n",
        "  try:\n",
        "    # Join path components\n",
        "    image_path = '/content/preprocessed_images/' + image_path\n",
        "\n",
        "    # Remove trailing '/'\n",
        "    image_path = tf.strings.regex_replace(image_path, '/.jpg$', '.jpg')\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_image(image, channels=1)\n",
        "    image = tf.image.resize_with_pad(image, target_height=128, target_width=128)\n",
        "    \n",
        "    return image, label\n",
        "  except tf.errors.NotFoundError:\n",
        "        # Skip the instance if the file is not found\n",
        "    return None, None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess_fn)\n",
        "dataset = dataset.filter(lambda image, label: image is not None and label is not None)\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "dataset = dataset.shuffle(buffer_size=len(images))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "# Split into training and test datasets\n",
        "train_dataset = dataset.take(int(0.9 * len(images)))\n",
        "test_dataset = dataset.skip(int(0.9 * len(images)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLN_8itrWCaJ"
      },
      "outputs": [],
      "source": [
        "epochs = 4\n",
        "\n",
        "# Clear the memory after each epoch\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=1,\n",
        "        validation_data=test_dataset,\n",
        "        callbacks=[mc, ec]\n",
        "    )\n",
        "\n",
        "   # Clear the memory\n",
        "    #clear_memory()\n",
        "\n",
        "model.save('/content/drive/MyDrive/ML2/ocr_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtwoGk-qyOJE"
      },
      "source": [
        "# Evaluation and Comparision\n",
        "\n",
        "I was not able to commpare my model with a pretrained one due to the complexity of the models. I was not able to find a model that would work exactly like mine. They either could only recognize single letters or they would need an additional model.\n",
        "I also tried using pytesseract, but I was not able to write a functioning evaluation code snippet for that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaznEeDuK3yS"
      },
      "outputs": [],
      "source": [
        "#load own model. If training isn't executed, it will download the model from google drive\n",
        "if os.path.exists(\"/content/ocr_model.h5\"):\n",
        "  own_model=tf.keras.models.load_model(\"/content/ocr_model.h5\")\n",
        "else:\n",
        "  !gdown 1-6q7l5TIbGhSrBzoYdZYGWPUdRWAQUiN\n",
        "  own_model=tf.keras.models.load_model(\"/content/ocr_model_trained.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJTO4xRsyW-K"
      },
      "outputs": [],
      "source": [
        "eval_own=own_model.evaluate(test_dataset)\n",
        "print(f\"Evaluation of my model: \\n Test Loss: {eval_own[0]}\\n Test Accuracy:{eval_own[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Application of my model"
      ],
      "metadata": {
        "id": "wfKE7rSugb8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def preprocess_uploaded(image):\n",
        "  image = image.resize((128,128))\n",
        "  image = np.array(image)\n",
        "  image = image[:,:,:1]\n",
        "  image = image / 255.0\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  return image\n",
        "\n",
        "uploaded_file = list(files.upload().values())[0]\n",
        "image = Image.open(io.BytesIO(uploaded_file))\n",
        "preprocessed = preprocess_uploaded(image)\n",
        "predicted = own_model.predict(preprocessed)\n",
        "predicted = np.argmax(predicted, axis=1)\n",
        "result = label_encoder.inverse_transform(predicted)[0]\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qEI1hMSegigE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1LTJLfeyavaXGWEsUgOUEEcu2vi8QZDyR",
      "authorship_tag": "ABX9TyMLz2Kg6TGj9AIurcQvfKN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}